{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process.py #\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def data_preprocess(data, split_ratio):\n",
    "    _data = data\n",
    "    _split_ratio = split_ratio\n",
    "    n_rows = _data.shape[0] \t# 데이터의 행 수\n",
    "    n_cols = _data.shape[1]   # 데이터의 열 수\n",
    "    data_c_open = _data[['Open']] # 개시 가격 데이터 로드\n",
    "    tradePrice = np.array(data_c_open[1:]) # 다음날의 개시가격 로드\n",
    "    _data = _data[:-1]\n",
    "\n",
    "    if len(tradePrice) == len(_data):   # 행수가 동일하면 데이터를 접합\n",
    "        _data = _data.loc[:,['Open','High','Low','Close','Volume']] \n",
    "        _data['tradePrice'] = tradePrice # 당일의 개시일 및 거래가격 \n",
    "        _data['cash'] = 100000.   # 초기 현금 100,000원으로 설정\n",
    "        _data['stockValue'] = 0.   # 초기 주식량 0으로 설정\n",
    "        _data = np.array(_data)\n",
    "        n_train = int(np.round(_split_ratio * n_rows))   # 훈련 데이터의 수\n",
    "        n_test = n_rows - n_train    # 검증 데이터의 수\n",
    "        train = _data[:n_train]    # 훈련 데이터 셋\n",
    "        test = _data[-n_test:]    # 검증 데이터 셋\n",
    "        return train, test\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment.py #\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# 상태의 단계\n",
    "N_TIMESTEPS = 5\n",
    "# 매번 거래되는 주식 수\n",
    "ORDER_SIZE = 10\n",
    "\n",
    "class StockEnv(Environment):\n",
    "    def __init__(self, data):\n",
    "        self.xdata = data\n",
    "        self.reset()  # 초기화 환경\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'StockEnvironment'\n",
    "\n",
    "    def close(self):\n",
    "        print('stock over') # 환경을 닫음. 추후 다른 메소드 호출이 가능하지 않음\n",
    "\n",
    "    def seed(self, seed):\n",
    "        return None  # 환경의 난수를 지정된 값으로 설정\n",
    "\n",
    "    def reset(self):  # 환경을 재설정하고 새 에피소드를 설정\n",
    "        self.sample_size = len(self.xdata)    # 샘플수 설정\n",
    "        self.step_counter = 0     # 훈련 초기화\n",
    "        self.n_timesteps = N_TIMESTEPS   # 상태 단계 초기화\n",
    "        self.order_size = ORDER_SIZE   # 단일 거래 번호 초기화\n",
    "        self.n_actions = len(self.actions)    # 초기화 된 action 수\n",
    "        self.reward = 0.    # 보상 초기화\n",
    "        self.current_state = self.states     # 상태 초기화\n",
    "        self.next_states = self.xdata[self.step_counter + 1:  \t\t\t\t+ self.step_counter + self.n_timesteps + 1]\n",
    "        self.tradePrice = self.states[-1, 5]   # 거래 당일 개시가격(21일)\n",
    "        self.cal_value_price = self.states[-1, 3] # 기록 마감 가격(20일), 주식 가치 측정\n",
    "        self.stock_amount = 0. # 보유한 주식 수량 초기화\n",
    "        self.stock_value = 0. # 주식값 초기화\n",
    "        self.cash_hold = 100000.   # 현재 현금\n",
    "        self.current_value = self.cash_hold + self.stock_value   # 현재 재산초기화 \n",
    "        self.past_value = 100000.   # 과거 재산의 총가치 초기화\n",
    "        self.done = False   # 현재 최종 상태에 있는지 여부\n",
    "        \n",
    "    def execute(self, action):  # 행동을 실행하고 다음 상태 및 보상을 관찰\n",
    "        self.past_value = self.current_value   # 과거 재산을 현재 재산으로 업데이트\n",
    "        self.cal_value_price = self.states[-1, 3]   # 주식 가치 계산\n",
    "        self.tradePrice = self.states[-1, 5]   # 거래 주문\n",
    "        \n",
    "        if action == 1: # 구매\n",
    "            # 현금 총액 업데이트\n",
    "            self.cash_hold = self.cash_hold - self.tradePrice * self.order_size \n",
    "            # 주식 수량 업데이트\n",
    "            self.stock_amount = self.stock_amount + self.order_size\n",
    "            \n",
    "        elif action == 2: # 판매\n",
    "            # 현금 총액 업데이트\n",
    "            self.cash_hold = self.cash_hold + self.tradePrice * self.order_size\n",
    "            # 주식 수량 업데이트\n",
    "            self.stock_amount = self.stock_amount - self.order_size  \n",
    "        # 돈의 가치를 고려하고 현금의 총가치를 업데이트\n",
    "        self.cash_hold = 0.9997 * self.cash_hold \n",
    "\n",
    "        # 상태 업데이트\n",
    "        if self.step_counter + self.n_timesteps + 1 < self.sample_size: \n",
    "            self.done = False\n",
    "            self.next_states = self.xdata[self.step_counter + 1 : self.step_counter +  \t\t\t    self.n_timesteps + 1]\n",
    "            # 주식 가치를 계산하는데 사용된느 다음날의 종가를 계산\n",
    "            self.stock_value = self.next_states[-1, 3] * self.stock_amount\n",
    "            # 당일 현금 및 재고 값 업데이트\n",
    "            self.xdata[self.step_counter + self.n_timesteps, 6] = self.cash_hold\n",
    "            self.xdata[self.step_counter + self.n_timesteps, 7] = self.stock_value  \n",
    "        else:\n",
    "            self.done = True\n",
    "            self.next_states = self.states\n",
    "            # 당일의 종가가 계산되지 않았으므로, 거래 개시 시점의 주가를 계산함 \n",
    "            self.stock_value = self.next_states[-1, 5] * self.stock_amount\n",
    "            # 당일 현금 및 재고 값 업데이트\n",
    "            self.xdata[self.step_counter + self.n_timesteps, 6] = self.cash_hold\n",
    "            self.xdata[self.step_counter + self.n_timesteps, 7] = self.stock_value\n",
    "\n",
    "        # 자본의 총액 계산(현금 + 주식 가치)\n",
    "        self.current_value = self.cash_hold + self.stock_value\n",
    "        \n",
    "        # 보상 규칙\n",
    "        # 다음과 같은 상황에서는 최대 패널티를 내며 주식 거래를 일찍 종료：\n",
    "        #       1. 현금 < 0; \n",
    "        #       2. 자본금 < 70,000\n",
    "        #       3. 현금이 총 자본금의 30% 미만으로 유지될 때\n",
    "        #       4. 주식 구매 시 현금이 충분하지 않을 때\n",
    "        \n",
    "        if self.cash_hold <= 0 or self.current_value <= 70000. or \t\t\t\t(self.cash_hold <= (0.3 * self.current_value)):  \n",
    "            self.reward = self.reward - 1.\n",
    "\n",
    "        if self.stock_amount < 0 and ((-1 * self.stock_amount) > \t\t\t\t(0.7 * self.cash_hold / self.states[-1,5])):\n",
    "            self.reward = self.reward - 1.\n",
    "\n",
    "        # 보상 벌칙\n",
    "        self.reward = self.reward + 1. * (self.current_value - self.past_value) /  \t\t\t\tself.past_value\n",
    "\n",
    "        self.step_counter = self.step_counter + 1 # 학습 시간 업데이트        \n",
    "        return self.next_states, self.done, self.reward \n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        # 현재 시점에서 20개 이상 데이터를 취할수 있는 경우 현재 상태를 반환\n",
    "        if self.step_counter + self.n_timesteps < self.sample_size:\n",
    "            states = self.xdata[self.step_counter : self.step_counter +\t\t\t\t\t\t\t self.n_timesteps]\n",
    "            return states\n",
    "        # 현재 시점에서 20개 이상 데이터를 취할수 없는 경우 이전 상태를 반환\n",
    "        else:\n",
    "            # print(\"No More Data.\")\n",
    "            self.done = True\n",
    "            states = self.xdata[self.step_counter-1 : self.step_counter + \t\t\t\t\t\t\tself.n_timesteps-1]\n",
    "            return states\n",
    "\n",
    "    @property\n",
    "    def actions(self):   # action 공간을 반환\n",
    "        actions = [0, 1, 2]  # 세 가지 행동( 0:보류, 1:구매, 2:판매 )\n",
    "        return actions\n",
    "\n",
    "    @staticmethod\n",
    "    def from_spec(spec, kwargs):   # 환경을 기록\n",
    "        env = tensorforce.util.get_object(\n",
    "            obj=spec,\n",
    "            predefined_objects=tensorforce.environments.environments,\n",
    "            kwargs=kwargs)\n",
    "        assert isinstance(env, Environment)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN.py#\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# 파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.9\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 0.001\n",
    "TAU = 0.05 \n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env = env   # 학습환경 설정\n",
    "        self.memory  = deque(maxlen=2000)  # 초기화 매개변수 설정\n",
    "        self.gamma = GAMMA  \n",
    "        self.epsilon = EPSILON \n",
    "        self.epsilon_min = EPSILON_MIN  \n",
    "        self.epsilon_decay = EPSILON_DECAY  \n",
    "        self.learning_rate = LEARNING_RATE  \n",
    "        self.tau = TAU \n",
    "        self.eval_model = self.create_model()   # 평가 네트워크 생성\n",
    "        self.target_model = self.create_model()   # 타겟 네트워크 생성\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()   # 모델 생성\n",
    "        state_shape  = self.env.states.shape\n",
    "\n",
    "        # 첫번째 히든 레이어 설정\n",
    "        model.add(Dense(32, input_dim=state_shape[1], activation=\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # 두번째 히든 레이어 설정\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # 세번째 히든 레이어 설정\n",
    "        model.add(Dense(32, activation=\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # 출력 레이어 설정(soft-max)\n",
    "        model.add(Dense(self.env.n_actions, activation=\"linear\"))\n",
    "\n",
    "        # 네트워크 모델 생성\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay    # epsilon 없데이트\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.env.n_actions)\n",
    "        return np.argmax(self.eval_model.predict(state)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])  # 메모리 설정\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = BATCH_SIZE    # 배치 수 설정\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "        samples = random.sample(self.memory, batch_size)    # 랜덤 샘플링\n",
    "\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            # 타겟 모델을 사용해 예측하고 예측된 결과를 대상에 저장\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.eval_model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        # 평가 네트워크의 가중치 매개변수를 가져옴\n",
    "        eval_weights = self.eval_model.get_weights()\n",
    "        # 타겟 네트워크의 가중치 매개변수를 가져옴\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        # 타겟 네트워크의 가중치 매개변수를 하나씩 업데이트\n",
    "        for i in range(len(target_weights)):\n",
    "            # 평가 및 타겟 네트워크의 가중치 매개변수값을 tau로 조정하여 업데이트\n",
    "            target_weights[i] = eval_weights[i] * self.tau + target_weights[i] * \t\t\t\t\t\t\t     (1 - self.tau)\n",
    "        # 업데이트된 가중치 매개변수를 타겟 네트워크로 설정\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        \n",
    "    def save_model(self, fn):\n",
    "        self.eval_model.save(fn)    # 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer.py#\n",
    "\n",
    "import Environment\n",
    "import DQN\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "GAMMA = 0.9   # 보상 감소 비율\n",
    "EPSILON = 1.0   # EPSILON 기본값 설정\n",
    "EPSILON_MIN = 0.01   # 최소 EPSILON 설정\n",
    "EPSILON_DECAY = 0.995   # EPSILON 감소 비율\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self):\n",
    "        # 모델을 사용하기 전 이전 모델이 차지한 메모리를 삭제\n",
    "        K.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 보상 감소 비율 초기화\n",
    "        self.gamma = GAMMA\n",
    "        # epsilon 초기화\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_min = EPSILON_MIN  \n",
    "        self.epsilon_decay = EPSILON_DECAY  \n",
    "        self.train_success = False\n",
    "        \n",
    "        # 훈련을 마친 뒤, 테스트를 위해 최종 훈련된 모델의 이름을 반환\n",
    "        self.model_name = ''\n",
    "        \n",
    "    def trainer(self, symbol, env, epochs):\n",
    "        self.env = env   # 액세스 설정\n",
    "        self.dqn_agent = DQN.DQN(env=self.env)   # DQN Agent 초기화\n",
    "        self.epochs = epochs   # epoch 초기화\n",
    "        \n",
    "        self.epoch_len = self.env.sample_size – self.env.n_timesteps  # 훈련 단계 수 초기화\n",
    "        \n",
    "        for epoch in range(self.epochs):   # 훈련 시작\n",
    "            self.env.reset()   # 훈련 환경 초기화\n",
    "            cur_state = self.env.states   # 현재 상태 가져오기\n",
    "            fortune = list()   # 자본 기록 초기화\n",
    "            cash = list()   # 현금 기록 초기화\n",
    "            act = list()   # action 기록 초기화\n",
    "            re = list()   # 보상기록 초기화\n",
    "            \n",
    "            for step in range(self.epoch_len):\n",
    "                action = self.dqn_agent.act(cur_state) # 현재 상태에 따라 action을 선택\n",
    "                new_state, done, reward = self.env.execute(action) # 상태 반환\n",
    "                act.append(action)   # 반환된 action을 action 기록에 추가\n",
    "                re.append(reward)   # 반환된 보상을 보상 기록에 추가\n",
    "                cash.append(new_state[-1,6])   # 반환된 현금을 현금 기록에 추가\n",
    "                \n",
    "                # 현재 상태의 총 자본 가치를 기록하고 자본 기록에 추가\n",
    "                _fortune = new_state[-1,6] + new_state[-1,7]\n",
    "                fortune.append(_fortune)\n",
    "\n",
    "                # 기억 메모리에 저장\n",
    "                self.dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "                self.dqn_agent.replay() \n",
    "                if step > 20:\n",
    "                    self.dqn_agent.target_train()   # 모델 매개변수 업데이트\n",
    "                cur_state = new_state            # 다음 상태 가져오기\n",
    "\n",
    "                # 종료상태인 경우 훈련을 종류하고 그렇지 않으면 훈련을 계속\n",
    "                if done:\n",
    "                    # 모델 훈련 완료시 모델을 저장\n",
    "                    if fortune[-1] >= 120000. and cash[-1] >= 0.:\n",
    "                        self.train_success = True\n",
    "                        self.model_name = \"success-model-{}-{}.h5“\n",
    "\t\t\t\t\t\t.format(symbol, epoch)\n",
    "                        self.dqn_agent.save_model(self.model_name)\n",
    "                        \n",
    "                    # 모델 훈련 미완료시 모델 제외\n",
    "                    else:\n",
    "                        self.train_success = False\n",
    "                        self.model_name = \"train-model-{}-{}.h5\".\n",
    "\t\t\t\t\t\t   format(symbol, epoch)\n",
    "                        self.dqn_agent.save_model(self.model_name)\n",
    "                    break\n",
    "            print(\"Epoch {}: Fortune-{}, Cash-{}, Reward-{}\".format(str(epoch),\n",
    "\t\tfortune[-1], cash[-1], re[-1]))    \n",
    "        return self.model_name   # 모든 훈련을 마친 후 모델명 변환   \n",
    "                       \n",
    "    def tester(self, env, model_name):\n",
    "        self.env = env   # 검증 데이터를 기반으로 환경 초기화\n",
    "        self.epoch_len = self.env.sample_size – self.env.n_timesteps   \n",
    "        self.model_name = model_name   # 모델 이름 초기화\n",
    "        self.test_model = load_model(self.model_name)   # 검증할 모델 불러오기\n",
    "        self.env.reset()   # 검증 환경 초기화\n",
    "        cur_state = self.env.states   # 현재상태 가져오기\n",
    "        fortune = list()   # 자본 기록 초기화\n",
    "        cash = list()   # 현금 기록 초기화\n",
    "        act = list()   # action 기록 초기화\n",
    "        re = list()   # 보상 기록 초기화\n",
    "        \n",
    "        for step in range(self.epoch_len):    # 검증 시작\n",
    "            self.epsilon *= self.epsilon_decay   # epsilon 업데이트\n",
    "            \n",
    "            # epsilon과 epsilon_min 중 큰 값으로 epsilon 업데이트\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "            \n",
    "            # 발생시킨 난수가 epsilon보다 작으면, 무작위로 action을 선택\n",
    "            if np.random.random() < self.epsilon:\n",
    "                action = np.random.randint(0, self.env.n_actions)\n",
    "                \n",
    "            # 발생시킨 난수가 epsilon보다 크거나 같으면 검증 모델에 따라 action을 선택\n",
    "            else:\n",
    "                action = np.argmax(self.test_model.predict(cur_state)[0])\n",
    "\n",
    "            # action 실행에 따른 상태 반환\n",
    "            new_state, done, reward = self.env.execute(action)\n",
    "            act.append(action)   # 반환된 action을 action 기록에 추가\n",
    "            re.append(reward)   # 반환된 보상을 보상 기록에 추가\n",
    "            cash.append(new_state[-1,6])  # 반환된 현금을 현금 기록에 추가  \n",
    "            \n",
    "            # 현재 상태의 총 자본 가치를 기록하고 자본 기록에 추가  \n",
    "            _fortune = new_state[-1,6] + new_state[-1,7]\n",
    "            fortune.append(_fortune)\n",
    "            cur_state = new_state   # 다음 상태 가져오기\n",
    "    \n",
    "            if done:    # 검증 종료 \n",
    "                break\n",
    "        \n",
    "        print(\"Test Result: Fortune-{}, Cash-{}, Reward-{}\".format(fortune[-1], \t     cash[-1], re[-1]))          \n",
    "\n",
    "        return fortune, act, re, cash # 결과 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main.py#\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import Preprocess\n",
    "import Environment\n",
    "import DQN\n",
    "import Trainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global _symbol\n",
    "global _split_ratio\n",
    "global _epochs\n",
    "\n",
    "_symbol = \"AAPL\"\n",
    "_split_ratio = 0.8\n",
    "_epochs = 5\n",
    "\n",
    "data_ex = pd.read_csv(\"apple.csv\") #apple 주식데이터 2016년 3월 ~ 2018년 3월\n",
    "\n",
    "# 훈련용, 검증용 데이터 분리\n",
    "train, test = Preprocess.data_preprocess(data_ex, _split_ratio)\n",
    "\n",
    "# 훈련 및 검증 환경 생성\n",
    "env_train = Environment.StockEnv(train)\n",
    "env_test = Environment.StockEnv(test)\n",
    "runner = Trainer.Runner()\n",
    "\n",
    "# DQN을 훈련시키고, 훈련된 모델로 돌아가 최종 결과를 업데이트\n",
    "trained_model = runner.trainer(_symbol, env_train, _epochs)\n",
    "\n",
    "# 훈련 된 trained_Q를 사용하여 테스트 데이터를 분석하고 예측 된 최종 거래 행동을 제공\n",
    "fortune, act, reward, cash = runner.tester(env_test, trained_model)\n",
    "print(\"fortune:{},act:{},reward:{},cash:{}\".format(fortune[-1], act[-1], reward[-1], cash[-1]))\n",
    "\n",
    "print(fortune)   # 설정한 기간(100일) 동안의 재산\n",
    "print(act)       # 설정한 기간(100일) 동안의 행동(1= 매수, 2= 매도, 3 = 보류)\n",
    "print(reward)   # 설정한 기간(100일) 동안의 보상\n",
    "print(cash)     # 설정한 기간(100일) 동안의 현금\n",
    "close_price = test[5:, 3] #검증용 데이터의 종가 정보를 가져옴\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(len(close_price)), close_price)\n",
    "ax.set_xlim((0, len(close_price)))\n",
    "ax.set_ylim((np.min(close_price), np.max(close_price)))\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Close Price\")\n",
    "ax.set_title('Trade Point predicted by DQN Trader')\n",
    "\n",
    "# 검증용 데이터의 종가 정보에 따른 강화학습 모델의 매수/매도 행동을 확인\n",
    "# 빨간색: 매수\n",
    "# 초록색: 매도\n",
    "\n",
    "for i in range(len(act)):\n",
    "    if act[i] == 1:\n",
    "        ax.scatter(x=i, y=close_price[i], c='r', marker='o', linewidths=0, label='Buy')\n",
    "    if act[i] == 2:\n",
    "        ax.scatter(x=i, y=close_price[i], c='g', marker='o', linewidths=0, label='Sell')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
